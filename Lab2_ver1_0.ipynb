{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pzgWxSP7Gjq"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://visitseattle.org/events/page/'\n",
        "detail_links = []\n",
        "\n",
        "# Scrape the event detail links from the first 41 pages\n",
        "for i in range(1, 42):\n",
        "    url = base_url + str(i)\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Select the specific elements containing the event links\n",
        "    for link in soup.select('div.search-result-preview > div > h3 > a'):\n",
        "        detail_links.append(link['href'])\n",
        "\n",
        "# Optionally, save the links to a file for later use\n",
        "with open('detail_links.pkl', 'wb') as f:\n",
        "    pickle.dump(detail_links, f)\n"
      ],
      "metadata": {
        "id": "TlIj42Ph7IGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('detail_links.pkl', 'rb') as f:\n",
        "    detail_links = pickle.load(f)\n",
        "\n",
        "event_list = []\n",
        "\n",
        "for event_url in detail_links:\n",
        "    response = requests.get(event_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract event details\n",
        "    name = soup.select_one('.medium-6.columns.event-top > h1').text\n",
        "    date = soup.select_one('.medium-6.columns.event-top > h4 > span:nth-child(1)').text\n",
        "    location = soup.select_one('.medium-6.columns.event-top > h4 > span:nth-child(2)').text.strip()\n",
        "    event_type = soup.select_one('.medium-6.columns.event-top > a:nth-child(3)').text\n",
        "    region = soup.select_one('.medium-6.columns.event-top > a:nth-child(4)').text\n",
        "\n",
        "    # Append the extracted details to the event list\n",
        "    event_list.append({\n",
        "        'Name': name,\n",
        "        'Date': date,\n",
        "        'Location': location,\n",
        "        'Type': event_type,\n",
        "        'Region': region\n",
        "    })\n",
        "    time.sleep(1)  # Be respectful by not overloading the server\n",
        "\n",
        "pd.DataFrame(event_list).to_csv('event_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "etz2i-OB8Neg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract locations from the event list for weather data\n",
        "Location_list = [event['Location'] for event in event_list]\n",
        "\n",
        "# Initialize list for weather data\n",
        "weather_list = []\n",
        "\n",
        "# Fetch weather data for each location\n",
        "for location in Location_list:\n",
        "    location_query = f'{location}, Seattle, WA'\n",
        "    query_params = {\"q\": location_query, \"format\": \"jsonv2\"}\n",
        "    response = requests.get(\"https://nominatim.openstreetmap.org/search\", params=query_params)\n",
        "    location_data = response.json()\n",
        "\n",
        "    if location_data:\n",
        "        latitude = location_data[0]['lat']\n",
        "        longitude = location_data[0]['lon']\n",
        "        weather_url = f\"https://api.weather.gov/points/{latitude},{longitude}\"\n",
        "        weather_response = requests.get(weather_url)\n",
        "        weather_data = weather_response.json()\n",
        "\n",
        "        if 'properties' in weather_data:\n",
        "            forecast_url = weather_data['properties']['forecast']\n",
        "            forecast_response = requests.get(forecast_url)\n",
        "            forecast_data = forecast_response.json()\n",
        "\n",
        "            if 'properties' in forecast_data:\n",
        "                for period in forecast_data['properties']['periods']:\n",
        "                    if period['isDaytime']:\n",
        "                        weather_list.append({\n",
        "                            'Location': location,\n",
        "                            'Day': period['name'],\n",
        "                            'Temperature': period['temperature'],\n",
        "                            'ShortForecast': period['shortForecast']\n",
        "                        })\n",
        "    else:\n",
        "        weather_list.append({\n",
        "            'Location': location,\n",
        "            'Day': 'No data',\n",
        "            'Temperature': 'No data',\n",
        "            'ShortForecast': 'No data'\n",
        "        })\n",
        "\n",
        "# Save weather data to a CSV file\n",
        "pd.DataFrame(weather_list).to_csv('weather_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "wZbKy8jO-sE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
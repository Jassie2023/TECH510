{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n"
      ],
      "metadata": {
        "id": "hjjyft81RzrM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get(url)\n",
        "print(\"HTTP Status Code:\", res.status_code)\n",
        "html_content = res.text\n",
        "with open(\"seattleevents.html\", \"w\") as file:\n",
        "    file.write(html_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy91-IPYUls-",
        "outputId": "84238ae4-6c88-49bf-f5c8-42a9872072c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP Status Code: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "selector = \"div.search-result-preview > div > h3 > a\"\n",
        "a_eles = soup.select(selector)\n",
        "event_links = [x['href'] for x in a_eles]\n"
      ],
      "metadata": {
        "id": "hqXJuUm6UmSl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "event_links_set = set()\n",
        "\n",
        "for page in range(1, 42):\n",
        "    time.sleep(1)  # Pause to avoid overloading the server\n",
        "    url = f\"https://visitseattle.org/events/page/{page}\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        if '/events/' in a['href']:\n",
        "            event_links_set.add(a['href'])\n",
        "\n",
        "event_links = list(event_links_set)\n",
        "print(f\"Total unique links collected: {len(event_links)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDlZ3AeWUoN6",
        "outputId": "dc4d3f67-12c1-4481-f8a7-8cc55c49821b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique links collected: 453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "event_links_set = set()\n",
        "\n",
        "for page in range(1, 42):\n",
        "    time.sleep(1)  # Pause to avoid overloading the server\n",
        "    url = f\"https://visitseattle.org/events/page/{page}\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        if '/events/' in a['href']:\n",
        "            event_links_set.add(a['href'])\n",
        "\n",
        "event_links = list(event_links_set)\n",
        "print(f\"Total unique links collected: {len(event_links)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q0zureoUtTk",
        "outputId": "273e5bcd-c92b-4c7e-bca8-313e80131e3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique links collected: 453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape event details and get lat/lon for the location\n",
        "def scrape_event_details(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    name = soup.find('h1', class_='page-title').get_text(strip=True) if soup.find('h1', class_='page-title') else 'Not Available'\n",
        "    date = soup.find('h4').get_text(strip=True) if soup.find('h4') else 'Not Available'\n",
        "    location = soup.find('h4').find_next('span').get_text(strip=True) if soup.find('h4') else 'Not Available'\n",
        "    event_type = soup.find('a', class_='category').get_text(strip=True) if soup.find('a', class_='category') else 'Not Available'\n",
        "    region = soup.find_all('a', class_='category')[1].get_text(strip=True) if len(soup.find_all('a', class_='category')) > 1 else 'Not Available'\n",
        "    lat, lon = get_lat_lon(location)\n",
        "    return {\n",
        "        'Name': name,\n",
        "        'Date': date,\n",
        "        'Location': location,\n",
        "        'Type': event_type,\n",
        "        'Region': region,\n",
        "        'Latitude': lat,\n",
        "        'Longitude': lon\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Eymp2jtoUv5_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Function to get latitude and longitude for a given location\n",
        "def get_lat_lon(location):\n",
        "    base_url = \"https://nominatim.openstreetmap.org/search.php\"\n",
        "    query_params = {\n",
        "        \"q\": location,\n",
        "        \"format\": \"jsonv2\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=query_params)\n",
        "    data = response.json()\n",
        "\n",
        "    if data and isinstance(data, list) and len(data) > 0:\n",
        "        return data[0].get('lat'), data[0].get('lon')\n",
        "    else:\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "Z_ALGOVhWb7k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Function to get weather data based on latitude and longitude\n",
        "def get_weather_data(lat, lon):\n",
        "    point_url = f\"https://api.weather.gov/points/{lat},{lon}\"\n",
        "    point_res = requests.get(point_url)\n",
        "\n",
        "    if point_res.status_code == 200:\n",
        "        point_data = point_res.json()\n",
        "        if 'properties' in point_data and 'forecast' in point_data['properties']:\n",
        "            forecast_url = point_data['properties']['forecast']\n",
        "            forecast_res = requests.get(forecast_url)\n",
        "            forecast_data = forecast_res.json()\n",
        "\n",
        "            if 'properties' in forecast_data and 'periods' in forecast_data['properties']:\n",
        "                periods = forecast_data['properties']['periods']\n",
        "                return periods\n",
        "            else:\n",
        "                return 'Forecast data not available'\n",
        "        else:\n",
        "            return 'Forecast URL not found'\n",
        "    else:\n",
        "        return 'Point data not available'\n"
      ],
      "metadata": {
        "id": "Y35V7IpDWmxl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "event_details = []\n",
        "\n",
        "for url in event_links:\n",
        "    time.sleep(1)\n",
        "    details = scrape_event_details(url)\n",
        "    if details['Latitude'] and details['Longitude']:\n",
        "        weather_forecast = get_weather_data(details['Latitude'], details['Longitude'])\n",
        "        details['Weather'] = weather_forecast\n",
        "    else:\n",
        "        details['Weather'] = 'Weather data not available'\n",
        "    event_details.append(details)\n",
        "\n",
        "# Write the data to a CSV file\n",
        "csv_file = 'events.csv'\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=['Name', 'Date', 'Location', 'Type', 'Region', 'Latitude', 'Longitude', 'Weather'])\n",
        "    writer.writeheader()\n",
        "    for event in event_details:\n",
        "        writer.writerow(event)\n",
        "\n",
        "print(f\"Data written to {csv_file}\")\n"
      ],
      "metadata": {
        "id": "k7rXN7K0UzAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}